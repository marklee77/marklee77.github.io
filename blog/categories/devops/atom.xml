<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | e-Enquiry]]></title>
  <link href="http://blog.stillwell.me/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://blog.stillwell.me/"/>
  <updated>2015-09-07T16:16:40+01:00</updated>
  <id>http://blog.stillwell.me/</id>
  <author>
    <name><![CDATA[Mark Stillwell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ansible Multi-OS Playbooks]]></title>
    <link href="http://blog.stillwell.me/blog/2015/06/09/multi-environment-deployments-with-ansible/"/>
    <updated>2015-06-09T13:26:28+01:00</updated>
    <id>http://blog.stillwell.me/blog/2015/06/09/multi-environment-deployments-with-ansible</id>
    <content type="html"><![CDATA[<p></p>

<p>Lately I&rsquo;ve been tinkering with some of my Ansible roles to improve the support
for multiple-environment deployments. Previously, I&rsquo;d been using a somewhat
naive approach of simply including environment-specific task lists using
conditionals in tasks/main.yml like so:</p>

<pre><code class="yaml">- include: ubuntu-install.yml
  when: ansible_distribution == "Ubuntu"

- include: centos-install.yml
  when: ansible_distribution == "CentOS"
</code></pre>

<p>However there are a number of problems with this approach: 1) it requires
keeping and maintaining separate task lists for each environment, 2)
deployments to different environments are done sequentially rather than in
parallel, and 3) it pollutes the output from ansible with lots of skipped tasks
for unused environments.</p>

<p>In general, it is better to use environment-specific variables with the same
task list for different environments, this reduces the maintenance overhead,
allows for parallel deployment, and reduces the size of the output.</p>

<!--more-->


<p>Environment-specific variables can be loaded by using &ldquo;include_vars&rdquo; and
&ldquo;with_first_found&rdquo; as in the following preamble:</p>

<pre><code class="yaml">- include: ubuntu-install.yml
- name: gather os specific variables
  include_vars: "{{ item }}"
  with_first_found:
    - files:
        - "{{ ansible_distribution|lower }}-{{
              ansible_distribution_version }}.yml"
        - "{{ ansible_distribution|lower }}-{{
              ansible_distribution_release }}.yml"
        - "{{ ansible_distribution|lower }}-{{
              ansible_distribution_major_version }}.yml"
        - "{{ ansible_distribution|lower }}.yml"
        - "{{ ansible_os_family|lower }}.yml"
        - defaults.yml
      paths:
        - ../vars
</code></pre>

<p>Note that this allows for graceful fallback to support different environments
in the same category; For an Ubuntu Trusty system, Ansible would look in the
vars/ subdirectory, and load the first vars file found from the following
sequence: &ldquo;ubuntu-14.04.yml&rdquo;, &ldquo;ubuntu-trusty.yml&rdquo;, &ldquo;ubuntu-14.yml&rdquo;,
&ldquo;ubuntu.yml&rdquo;, and, finally, &ldquo;debian.yml&rdquo;.</p>

<p>There is still a problem that different environments may require slightly
different build steps. Consider the steps for installing nova compute on
different operating systems. On Ubuntu trusty and precise, the steps are
basically the same, <em>except</em> that on precise it is necessary to first enable
the ubuntu cloud archive. On CentOS, not only are the package names different,
but we have to use the yum package manager instead of apt. Also, there are
a number of prerequisite packages that need to be in place before even trying
to install anything from openstack.</p>

<p>There aren&rsquo;t always perfect answers for every situation, but if the steps for
the different environments can be represented at a reasonable level of
abstraction then it may be possible to take advantage of the &ldquo;action&rdquo; keyword
to come up with a solution. For example, rather than &ldquo;install apt packages&rdquo; and
&ldquo;install rpms&rdquo;, a generic &ldquo;install packages&rdquo; step that could be run in parallel
on different environments might look like this:</p>

<pre><code class="yaml">- name: ensure required packages are installed
  action: "{{ package_info.pkg_mgr }}"
  args: package_info.args
  with_items: package_info.pkgs
  when: package_info.pkgs|length &gt; 0
</code></pre>

<p>The &ldquo;when&rdquo; keyword above is critical for steps where some environments may need
to install a set of packages, but others won&rsquo;t. For several of my roles there
are steps that look like the above, except there is also
a &ldquo;package_info.pre_pkgs&rdquo; step to install prerequisites, and it isn&rsquo;t hard to
imagine that for some deployments it might be necessary to have
a &ldquo;package_info.post_pkgs&rdquo; as well.</p>

<p>I could include more details, but if you&rsquo;ve gotten this far then you may as well
take a look at some of the multi-environment roles that I have on github:</p>

<ul>
<li><a href="https://github.com/marklee77/ansible-role-docker">ansible-role-docker</a></li>
<li><a href="https://github.com/marklee77/ansible-role-keystone">ansible-role-keystone</a></li>
<li><a href="https://github.com/marklee77/ansible-role-nova-compute">ansible-role-nova-compute</a></li>
</ul>


<p>The docker role supports a wide variety of environments, the keystone role only
supports Ubuntu trusty and precise (though if someone wanted to submit a pull
request I wouldn&rsquo;t complain), while the ansible-role-nova-compute role supports
Ubuntu trusty and CentOS 6.5 (due to a requirement at work). If anyone has any
criticisms or suggestions for a better way of doing things, then please feel
free to email me or comment below.</p>

<p></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ansible Deployment of Ansible-provisioned Docker Containers]]></title>
    <link href="http://blog.stillwell.me/blog/2014/10/05/ansible-deployment-of-ansible-provisioned-docker-containers/"/>
    <updated>2014-10-05T20:58:26+01:00</updated>
    <id>http://blog.stillwell.me/blog/2014/10/05/ansible-deployment-of-ansible-provisioned-docker-containers</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been working with <a href="http://ansible.com">Ansible</a> quite a lot for the past
year, but until this weekend I really hadn&rsquo;t had much of a chance to get
acquainted with <a href="http://docker.io">Docker</a>. There&rsquo;s a lot of talk on the
Internet about how these two different pieces of technology &ldquo;naturally&rdquo;
compliment each other, but clearly there&rsquo;s also a lot of confusion as well, and
there are still a few hard interface problems that need to be sorted out.</p>

<p>When I first hear that Ansible could be used to both provision and deploy
Docker containers, I naturally assumed that this meant I&rsquo;d be able to adapt my
existing Ansible workflow, possibly with the addition of some new modules to
use, and have access to the power of Docker, but this hasn&rsquo;t proven to be the
case <em>at all</em>.</p>

<p>In fact, the two functionalities, deployment and provisioning, are completely
disconnected from each other. To build images and deploy containers there are
two new modules: docker_image and docker, that work pretty much like you&rsquo;d
expect. It should be noted that the docker_image module can really only be used
to build images using existing Dockerfiles, and in fact there is little that it
can to do actually affect the internal workings of the image. To provision,
that is configure, an image, an entirely different approach is required: the
playbook or role needs to be copied into the container filesystem and then run
as a deployment to localhost. The problem with this, of course, is that this
makes it very difficult to coordinate configuration and state information. This
probably has a lot to do with the approach to containerized deployment
emphasized by Docker, wherein images are relatively static, and configuration
is done at deploy time by running scripts in linked containers. This may even be
the best way to do things, but it does make it difficult to adapt existing
playbooks and roles to work with docker.</p>

<!--more-->


<p>For the past few days I&rsquo;d been contemplating an approach based on three
separate git repositories for each project: one repository for the ansible
playbook to do the provisioning, one repository for a Dockerfile that pulls in
the first playbook to build the image, and a final repository for the ansible
playbook that manages the deployment to remote target systems. This is not
completely unreasonable for small projects, but it would quickly become
unwieldy for something with a lot of moving parts and subprojects, like, say,
<a href="http://openstack.org">OpenStack</a>. At some point it occurred to me that the
repositories containing the configuration playbook and Dockerfile could be
combined, and from there it was a small (but fiddly and error-prone) step to
figure out an approach that would let me keep every sub project in a single
repository.</p>

<p>My first project of this type is an extension of my existing role to deploy
<a href="http://mariadb.org">MariaDB</a> on <a href="http://ubuntu.com">Ubuntu</a>:
<a href="https://github.com/marklee77/ansible-role-mariadb">https://github.com/marklee77/ansible-role-mariadb</a>. This project is
organized as an ansible role, suitable for inclusion in larger projects. By
default, the role deploys MariaDB in the traditional way, but if Docker is
available on the target then the role can perform a sort of ju-jitsu maneuver,
copying itself to the target system and then instructing the target to use the
&ldquo;standard&rdquo; mode of the remote copy to build a local image. There are still
a few things to be worked out, like the best way to manage persistent state and
ensure that containers are restarted on reboots, but I think it&rsquo;s a clever bit
of work and I&rsquo;m really quite happy with it.</p>

<p>Suggestions, comments, and (on-topic) rants are welcome below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DevOps, Systems Administration, and the Culture of Fear]]></title>
    <link href="http://blog.stillwell.me/blog/2014/09/29/a-devops-approach-with-ansible-vagrant-and-docker/"/>
    <updated>2014-09-29T13:09:28+01:00</updated>
    <id>http://blog.stillwell.me/blog/2014/09/29/a-devops-approach-with-ansible-vagrant-and-docker</id>
    <content type="html"><![CDATA[<p>Today at the <a href="http://www.harness-project.eu">HARNESS project</a> integration team
meeting I gave a brief
<a href="http://www.slideshare.net/marklee1977/devops-39666655">presentation</a> wherein I
discussed <a href="http://en.wikipedia.org/wiki/DevOps">DevOps</a> and some of my favorite
tools, namely <a href="http://ansible.com">Ansible</a>, <a href="http://vagrantup.com">Vagrant</a>,
and <a href="http://docker.io">Docker</a>.</p>

<p>The central thesis of my talk was that a modern devops approach, based in
version-controlled configuration management implemented using industry-standard
deployment tools and making use of continuous integration and rolling updates
is a must for creating robust distributed systems with lots of moving parts.
Interactive configuration of cloud systems and virtual machines leads to
unreproducible, poorly documented, error-prone &ldquo;brittle&rdquo; systems. It is the
equivalent of collaborative authoring of documents by emailing word processor
files, or software version control by having a backup/ directory containing
numbered/dated zip archives. This leads to a &ldquo;culture of fear&rdquo; wherein people
are afraid to try to fix known problems for fear of &ldquo;making things worse&rdquo;, and
every roll out of of a new api or set of features becomes a dreaded nightmare.</p>

<p>Overall, the talk was well received. We also discussed the Vagrant workflow, and
how, while Docker can be used to provision containers that function the same
way as &ldquo;lightweight&rdquo; virtual machines, ideally each container should really
only be running the minimum number of processes, following the single
responsibility principle.</p>
]]></content>
  </entry>
  
</feed>
