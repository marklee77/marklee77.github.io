<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Semanticweb | e-Enquiry]]></title>
  <link href="http://blog.stillwell.me/blog/categories/semanticweb/atom.xml" rel="self"/>
  <link href="http://blog.stillwell.me/"/>
  <updated>2015-07-22T00:29:46+01:00</updated>
  <id>http://blog.stillwell.me/</id>
  <author>
    <name><![CDATA[Mark Stillwell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux Distribution for Recomputable Experiments]]></title>
    <link href="http://blog.stillwell.me/blog/2013/09/12/linux-distribution-for-recomputable-experiments/"/>
    <updated>2013-09-12T13:00:00+01:00</updated>
    <id>http://blog.stillwell.me/blog/2013/09/12/linux-distribution-for-recomputable-experiments</id>
    <content type="html"><![CDATA[<p>I had a great time yesterday at the
<a href="http://www.software.ac.uk/workshop-research-software-engineers">Workshop for Research Software Engineers</a>
put on by the
<a href="http://www.software.ac.uk">Sustainable Software Institute</a> and
hosted at the <a href="http://www.oerc.ox.ac.uk/">Oxford e-Research Centre</a>.
While there, I had an interesting conversation with
<a href="http://ipg.host.cs.st-andrews.ac.uk/">Ian Gent</a>, writer of
&ldquo;<a href="http://recomputation.org/">The Recomputation Manifesto</a>&rdquo;. You
should head over to the site and read some of Ian&rsquo;s articles, particularly
<a href="http://www.software.ac.uk/blog/2013-07-09-recomputation-manifesto">this one</a>,
as he&rsquo;s put quite a lot of thought into the topic. The gist of the
idea is as follows (apologies Ian for any misunderstandings, you should really
go to his site after this one): 1) computational experiments are only valuable
if they can be verified and validated, 2) in theory, it should be fairly easy
to make computational science experiments, particularly small-scale computer
science experiments, perfectly repeatable for all time, 3) in practice this is
never/rarely done and reproduction/verification is really hard, 4) the best or
possibly only way to accomplish this goal is to make sure that the entire
environment is reproducible by packaging it in a virtual machine.</p>

<p>I have a few thoughts of my own on how we can better accomplish these goals
after the break. The implementation of these ideas should be fairly simple and
basically add up to extending or developing a few system utilities and
systematically archiving distributions and updates on a service like
<a href="http://figshare.com">figshare</a>, but I believe that the benefits to
the cause of improving the reproducibility of computational experiments would be
enormous.</p>

<!--more-->


<p>Ian has spent some time working with <a
href="http://vagrantup.com">Vagrant</a>, a user friendly front end to
<a href="http://virtualbox.org">VirtualBox</a> (and now some other hypervisors)
that allows users to create custom virtual machines from a few standard base VM
images. Vagrant has some really nice features, like tracking all of the
configuration information in a text file that can be managed through a version
control system like <a href="http://git-scm.com">git</a>,
<a href="http://bazaar.canonical.com">bzr</a> or
<a href="http://darcs.net/">darcs</a>. Vagrant also plays nicely with some of
the more popular devops configuration management systems (including
<a href="http://puppetlabs.com">puppet</a>,
<a href="http://www.opscode.com/chef/">chef</a>, and
<a href="http://ansibleworks.com">ansible</a>), automatically configures file
synchronization between the working directory on the host an VM, and sets up
private networking between the host and one or more virtual machines in
a collection, so it&rsquo;s easy to ssh in and make network connections between
related machines.</p>

<p>I think that Vagrant is a great step forward in making it easier for
researchers to work with virtual machines and integrate them into their
workflows, but there are a few more things that need to be done. For one thing,
Vagrant is strongly dependent on the VirtualBox &ldquo;Guest Additons&rdquo; to
interact with the hypervisor from the VM. There is some support for different
providers now, but Vagrant treats each of these as a special case. Really,
there needs to be some standardization of this interface, so that the same VM
image can be reliably instantiated on any hypervisor and work with tools like
Vagrant transparently.</p>

<p>Another problem is that just virtualizing your experiment doesn&rsquo;t necessarily
make it easier to understand your methodology. You can create a VM blob and
archive that, even upload it to figshare or a similar service, register it with
<a href="http://datacite.org">DataCite</a> and give it a
<a href="http://www.doi.org/">DOI</a>, and that would at least solve the problem
of re-running the same experiment (for reasonably tractable experiments
anyway), but it would still be hard for others to know exactly what was done to
prepare the environment (you could have been hacking binaries with a hex editor
for all we know). It also suffers from the problem that it requires researchers
to pass around lots of mutli-gigabyte blobs that contain mostly-redundant data.
A much better plan is to start from a standard base image and then script in
all of the configuration and build steps (possibly within your Vagrantfile).</p>

<p>Unfortunately, current operating system distributions haven&rsquo;t been designed
with the idea of making it easy for researchers to create and archive
experiment environments in mind. For example, starting from the precise32.box
base box, it&rsquo;s perfectly reasonable to want to install some additional system
software (particularly build tools if you need them to compile source code),
but &ldquo;apt-get update&rdquo; will always get the latest software packages, and using
updated software could cause your experiment to build or run differently than
it did before. Neither <a href="http://debian.org">Debian</a> nor
<a href="http://ubuntu.com">Ubuntu</a> guarantee that every version of every
software package will always be available, making it necessary to keep your own
versions in a repository, which adds to complexity and overheads.</p>

<p>My idea is to archive full repository images at certain points in time so that
they have a DOI (or some other permanently reference-able
<a href="http://handle.net">handle</a> or
<a href="https://en.wikipedia.org/wiki/Permalink">permalink</a> if DOIs seem
inappropriate). Weekly or monthly updates could also be generated and given
their own DOI. These updates would not need to be full mirrors of the
repository, but could just contain primarily software packages that had been
changed since the last update. The updated software catalog could refer to
previous update DOIs or the base repository DOI for packages that hadn&rsquo;t
changed. In the end, the implementation of this idea would entail some changes
to the procedures for archiving software, and some extensions to apt-get (or
yum or yast) to work with DOIs instead of URLs. The payoff would be that users
of tools like Vagrant would be able to specify a specific version of an
operating system at a specific moment in time, forever, by simply giving the
correct DOI in their configuration (Vagrantfile or similar). It could be done
by interested parties (e.g., academics) independent of the distribution
providers, but there are questions about who would pay for storage and access,
and keeping everything reasonably up-to-date, so perhaps it makes the most
sense to approach the providers and get support there. It really wouldn&rsquo;t add
very much complexity to their existing processes.</p>

<p>One other idea I have is to extend tools like vagrant to better support
a paradigm of &ldquo;builder VMs&rdquo; and &ldquo;worker appliances&rdquo;. Basically, setting up an
experiment might require various specific versions of different tools,
particularly compilers, linkers, etcetera, that take up quite a lot of space.
These tools need to be downloaded from a specific version of a repository and
instantiated within a VM in order to replicate the required build environment,
but aren&rsquo;t needed to actually run the experiment. Instead of downloading
everything needed for a fully-interactive operating system complete with
build-tools onto the experiment-running VM, and then possibly distributing this
multi-gigabyte image to a cloud back-end, it makes a lot more sense to install
the build tools on a local-machine VM, and use this local VM to create
a minimal virtual appliance to actually run the experiment. Ideally, this
appliance should have only the code and data needed to run the experiment, and
could even be stripped of things like the standard shell utilities and login
tools. This would minimize both the size and complexity of the resulting VM
used to run experiments.</p>

<p>I&rsquo;m going to try to move forward on implementing some of these ideas over the
next few months, but I have classes to teach and papers to write, so it might
take a little while. If anyone wants to help out, or has some suggestions for
a better approach, then please email me or drop a comment below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Repetition of Research (Unintended)]]></title>
    <link href="http://blog.stillwell.me/blog/2013/08/03/repetition-of-research/"/>
    <updated>2013-08-03T16:32:50+01:00</updated>
    <id>http://blog.stillwell.me/blog/2013/08/03/repetition-of-research</id>
    <content type="html"><![CDATA[<p>Most of us are familiar with the classical narrative of ongoing scientific
progress: that each new discovery builds upon previous ones, creating an ever
upward-rising edifice of human knowledge shaped something like an inverted
pyramid. There&rsquo;s also an idea that, in the semi-distant past of a few hundred
years ago one person could know all the scientific knowledge of his (or her)
time, but today the vast and ever-expanding amount of information available
means that scientists must be much more specialized, and that as time passes
they will become ever more specialized.</p>

<p>There is some truth to these ideas, but there are problems as well: when new
knowledge is created, how is it added to the edifice? How do we make sure that
future scholars will know about it and properly reference it in their own works?
If a scientist must be incredibly specialized to advance knowledge, then what
does he (or she) do when just starting out? How does one choose a field of
research? And what happens when the funding for research into that area dries
up? Contrary to what we learned in grade school, a scientist cannot choose to
simply study, say, some obscure species of Peruvian moth and spend the next 40
years of summers in South America learning everything there is to know about it
without also spending some time justifying that decision to colleagues and
funding bodies.</p>

<!--more-->


<p>New scientific knowledge is published, generally in refereed conference
proceedings and/or journals, but sometimes also online in the form of research
reports or blog posts. The good thing about refereed publications is that they
have been read by the author&rsquo;s (or authors&#8217;) peers, who have rendered
a judgment that the work is of sufficient quality and novelty to merit adding
it to the official scientific literature. The bad thing is that this process is
slow and prone to errors&ndash;referees are human beings, and they work on
a voluntary basis, and they may only be partial experts on the topic of the
article in question. The fact of the matter is that it is extremely likely that
a lot of science is repeated, in fact that it makes sense that a lot of work
gets repeated since it actually takes less work to re-research some answers in
the lab (or, for mathematics, on the chalkboard) than it does to keep track of
every single research output produced by every researcher, everywhere, for all
time.</p>

<p>Let that sink in for a minute: There are costs associated with storing,
indexing, and searching vasts amounts of data. For some problems, for some
researchers, at some time, those costs may actually exceed the cost of simply
re-discovering the findings anew. An additional wrinkle is that the people
making the decision to embark on a new research project by definition do not
know how difficult it will be to properly research the problem, and are strongly
motivated <em>NOT</em> to discover that their proposed solution has already been
discovered by someone else (think: graduate students looking for interesting
thesis projects, or untenured junior faculty hoping to pad out their research
portfolio, or anyone submitting a grant proposal), so long as the omission is
not so completely obvious as to be embarrassing.</p>

<p>Consideration of the second point provides some more startling insights: yes,
researchers do become highly specialized in their knowledge, but this
specialization is <em>NOT</em> defined by an objective partitioning of the universe of
knowledge into discrete, manageable, non-overlapping subsets, but rather by
what journals or conferences they follow, who their colleagues are, and other
factors like their personal algorithms for sorting and selecting papers from
the constant flood of new information. The primary limitation on scientists is
how many papers they are physically and mentally capable of reading and
assimilating within a given timespan, and this value is likely a constant that
varies only somewhat from individual to individual. Further limiting the
potential for specialization is the fact that every paper ranges over a wide
variety of semi-related topics, in order to increase its chance of being deemed
sufficiently &ldquo;interesting&rdquo; and &ldquo;novel&rdquo; to warrant publication by a randomly
selected collection of &ldquo;peers&rdquo;. It is not only possible, but likely, that there
are many different groups of researchers who study similar problems, but never
know about each other because they follow different conferences, and use
differing terminology. Further, they will use different basic assumptions when
designing and evaluating experiments, having different standards for what
constitutes &ldquo;good&rdquo; or &ldquo;bad&rdquo; solutions to related problems. For an example of
this, I&rsquo;d like to highlight the almost complete disconnect between groups that
study scheduling algorithms for distributed systems, which take a very
&ldquo;mathematical&rdquo; and proof-oriented approach, falling back on studies of
heuristics for very abstract problem formulations when there are no reasonable
guaranteed algorithms, and groups that study &ldquo;autonomic computing&rdquo;, which take
a much more biologically inspired approach to what is essentially the same
problem, starting from a more common-sense (but less mathematically tractable)
approach to defining success or failure, relying first on heuristics, but often
falling back on biologically inspired approaches like ant-colony optimization
or genetic algorithms.</p>

<p>So, a lot of research gets repeated, and for some people in some situations,
that might not be an entirely bad thing. Still, the situation seems something
less than optimal. Hence, there have been movements later toward not only
greater sharing of research data, both before, and after publication, but also
better sharing of data through the use of standardized, extensible formats like
XML or JSON, and semantic markup of metadata. By using global- and
discipline-standardized <em>ontologies</em>, or grammars that define relationships
between syntactic primitives, it is possible to develop general purpose
repositories that nonetheless allow for complicated queries and
discipline-specific reasoning about results. By using these standards and
technologies and applying well-though-out policies for <em>research data
management</em>, it should be possible to lower the bar for indexing and searching
the existing body of scientific knowledge (which to those on the inside looks
much less like an edifice and more like a maelstrom).</p>

<p>I plan on covering both the development of data management policies and why they
are a good idea, and enabling technologies for the storage and semantic markup
of data in future posts.</p>
]]></content>
  </entry>
  
</feed>
